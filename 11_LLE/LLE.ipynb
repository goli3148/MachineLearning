{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "x = data.data\n",
    "y = data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and fit LLE model\n",
    "# lle = LocallyLinearEmbedding(n_neighbors=20, n_components=2)\n",
    "# X_transformed = lle.fit_transform(x)\n",
    "# print(X_transformed.shape)\n",
    "\n",
    "# # Visualize the results\n",
    "# plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y)\n",
    "# plt.title('LLE Projection of Iris Dataset')\n",
    "# plt.xlabel('Component 1')\n",
    "# plt.ylabel('Component 2')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"markdown prose w-full break-words dark:prose-invert dark\"><p>The \"Singular matrix\" error typically occurs when you try to compute the inverse of a singular or nearly singular matrix. In the context of Locally Linear Embedding (LLE) or similar algorithms, it often means that the local neighborhood for a particular point is not diverse enough, leading to a poorly conditioned matrix.</p><p>To address this issue, you can add a small regularization term to the diagonal of the matrix <code>G</code> before computing the inverse. This regularization helps stabilize the inversion process and avoid singularities. The modified line would look like this:</p><pre><div class=\"bg-black rounded-md\"><div class=\"flex items-center relative text-gray-200 bg-gray-800 dark:bg-token-surface-primary px-4 py-2 text-xs font-sans justify-between rounded-t-md\"></div><div class=\"p-4 overflow-y-auto\"><code class=\"!whitespace-pre hljs language-python\">G_inv = np.linalg.inv(G + <span class=\"hljs-number\">1e-4</span> * np.eye(G.shape[<span class=\"hljs-number\">0</span>]))\n",
    "</code></div></div></pre><p>Here, <code>1e-4</code> is a small value that you can adjust based on your specific dataset and requirements.</p><p>Feel free to experiment with different values for the regularization term (e.g., <code>1e-4</code>) to find a suitable value for your specific dataset. Adjusting this value can help balance the trade-off between stability and the influence of the regularization term.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocallyLinearEmbedding:\n",
    "    n_component: int\n",
    "    n_neighbours: int\n",
    "    data: np.ndarray\n",
    "    weghits: np.ndarray\n",
    "    eigenvectors: np.ndarray  # Store the eigenvectors from fit_transform\n",
    "    \n",
    "    def __init__(self, n_component, n_neighbours) -> None:\n",
    "        self.n_component = n_component\n",
    "        self.n_neighbours = n_neighbours\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.data = data\n",
    "        self.LinearReconustruction()\n",
    "        return self.LinearEmbedding()\n",
    "    \n",
    "    def LinearEmbedding(self):\n",
    "        w_size = self.weghits.shape[0]\n",
    "        M = np.eye(w_size) - self.weghits\n",
    "        M = M.T @ M\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(M)\n",
    "        indices = np.argsort(eigenvalues)[1:self.n_component+1]\n",
    "        self.eigenvectors = np.real(eigenvectors[:, indices])  \n",
    "        Y = self.eigenvectors\n",
    "        return Y\n",
    "    \n",
    "    def LinearReconustruction(self):\n",
    "        w = []  # weights matrix with n*n_neighbours size\n",
    "        ones = np.ones(self.n_neighbours).reshape(-1, 1)\n",
    "        for i in range(self.data.shape[0]):\n",
    "            d = self.data[i].reshape(-1, 1)\n",
    "            knn, knn_i = self.KNN(i)\n",
    "            G = d @ ones.T - knn.T\n",
    "            G = G.T @ G\n",
    "            G_inv = np.linalg.inv(G + 1e-4 * np.eye(G.shape[0]))\n",
    "            w_i = ((G_inv @ ones) / (ones.T @ G_inv @ ones)).squeeze()\n",
    "            w_i = self.ReconstructWeights(w_i, knn_i)\n",
    "            w.append(w_i)\n",
    "        self.weghits = np.array(w)\n",
    "        return w\n",
    "    \n",
    "    def ReconstructWeights(self, w, knn_i):\n",
    "        res = np.zeros((self.data.shape[0]))\n",
    "        for i in range(res.shape[0]):\n",
    "            for index in range(len(knn_i)):\n",
    "                if knn_i[index] == i:\n",
    "                    res[i] = w[index]\n",
    "                    break\n",
    "        return res\n",
    "    \n",
    "    def KNN(self, index):\n",
    "        neighbors_t = {}\n",
    "        for i in range(self.data.shape[0]):\n",
    "            if not i == index:\n",
    "                neighbors_t[i] = self.distance(self.data[i], self.data[index])\n",
    "        neighbors_t = dict(sorted(neighbors_t.items(), key=lambda item: item[1]))\n",
    "        neighbors = []\n",
    "        neighbors_i = []\n",
    "        for index, key in enumerate(neighbors_t):\n",
    "            neighbors.append(self.data[key])  # data of key\n",
    "            neighbors_i.append(key)  # key itself\n",
    "            if index + 1 == self.n_neighbours:\n",
    "                break\n",
    "        return np.array(neighbors), np.array(neighbors_i)\n",
    "    \n",
    "    def transform(self, new_data):\n",
    "        # Step 1: Find the KNN of the new data in the original data set\n",
    "        w = []\n",
    "        ones = np.ones(self.n_neighbours).reshape(-1, 1)\n",
    "        for d in new_data:\n",
    "            d = d.reshape(-1, 1)\n",
    "            knn, knn_i = self.KNN_new_point(d)\n",
    "            G = d @ ones.T - knn.T\n",
    "            G = G.T @ G\n",
    "            G_inv = np.linalg.inv(G + 1e-4 * np.eye(G.shape[0]))\n",
    "            w_i = ((G_inv @ ones) / (ones.T @ G_inv @ ones)).squeeze()\n",
    "            w_i = self.ReconstructWeights(w_i, knn_i)\n",
    "            w.append(w_i)\n",
    "        w_new = np.array(w)\n",
    "        \n",
    "        # Step 2: Use the eigenvectors from fit_transform to project the new data\n",
    "        Y_new = w_new @ self.eigenvectors\n",
    "        return Y_new\n",
    "    \n",
    "    def KNN_new_point(self, point):\n",
    "        neighbors_t = {}\n",
    "        for i in range(self.data.shape[0]):\n",
    "            neighbors_t[i] = self.distance(self.data[i], point.squeeze())\n",
    "        neighbors_t = dict(sorted(neighbors_t.items(), key=lambda item: item[1]))\n",
    "        neighbors = []\n",
    "        neighbors_i = []\n",
    "        for index, key in enumerate(neighbors_t):\n",
    "            neighbors.append(self.data[key])  # data of key\n",
    "            neighbors_i.append(key)  # key itself\n",
    "            if index + 1 == self.n_neighbours:\n",
    "                break\n",
    "        return np.array(neighbors), np.array(neighbors_i)\n",
    "    \n",
    "    def distance(self, data1, data2, sigma=0.001):\n",
    "        # return np.linalg.norm(data1 - data2)\n",
    "        return np.exp(-np.linalg.norm(data1 - data2)**2 / (2 * sigma**2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLE = LocallyLinearEmbedding(2, 3)\n",
    "# x_LLE = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\n",
    "# y_LLE = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\n",
    "# classes = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
    "# selected = 7\n",
    "# LLE.data = np.column_stack((x_LLE,y_LLE))\n",
    "# print(LLE.data[selected])\n",
    "# n, _=LLE.KNN(selected)\n",
    "# for i in n:\n",
    "#     print(i)\n",
    "#     plt.scatter(i[0], i[1], c='green', alpha=.8, zorder=100, label='neighbours')\n",
    "# plt.scatter(LLE.data[selected][0], LLE.data[selected][1], c='red', zorder=100, label='selected')\n",
    "# plt.scatter(x_LLE, y_LLE, c=classes, label='all points')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "lle = LocallyLinearEmbedding(n_component=3, n_neighbours=5)\n",
    "x_new_train = lle.fit_transform(x_train)\n",
    "x_new_test = lle.transform(x_test)\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_new_train, y_train)\n",
    "\n",
    "y_pred_train = gnb.predict(x_new_train)\n",
    "y_pred_test = gnb.predict(x_new_test)\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthiness: 0.9506696428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import trustworthiness\n",
    "\n",
    "def calculate_trustworthiness(X_high, X_low, n_neighbors=5):\n",
    "    score = trustworthiness(X_high, X_low, n_neighbors=n_neighbors)\n",
    "    return score\n",
    "\n",
    "# Example usage:\n",
    "trustworthiness_score = calculate_trustworthiness(x_train, x_new_train)\n",
    "print(f\"Trustworthiness: {trustworthiness_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Variance: 0.9297717828846278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def residual_variance(X_high, X_low):\n",
    "    # Calculate pairwise distances in high-dimensional and low-dimensional spaces\n",
    "    D_high = pairwise_distances(X_high)\n",
    "    D_low = pairwise_distances(X_low)\n",
    "\n",
    "    # Flatten the distance matrices\n",
    "    D_high_flat = D_high.flatten()\n",
    "    D_low_flat = D_low.flatten()\n",
    "\n",
    "    # Calculate the residual variance\n",
    "    variance_high = np.var(D_high_flat)\n",
    "    variance_diff = np.var(D_high_flat - D_low_flat)\n",
    "    residual_variance = variance_diff / variance_high\n",
    "\n",
    "    return residual_variance\n",
    "\n",
    "# Example usage:\n",
    "# X_high is the original high-dimensional data\n",
    "# X_low is the LLE-embedded data\n",
    "residual_variance_score = residual_variance(x_train, x_new_train)\n",
    "print(f\"Residual Variance: {residual_variance_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuity: 0.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def calculate_continuity(X_high, X_low, n_neighbors=5):\n",
    "    nn_high = NearestNeighbors(n_neighbors=n_neighbors).fit(X_high)\n",
    "    nn_low = NearestNeighbors(n_neighbors=n_neighbors).fit(X_low)\n",
    "\n",
    "    high_neighbors = nn_high.kneighbors(return_distance=False)\n",
    "    low_neighbors = nn_low.kneighbors(return_distance=False)\n",
    "\n",
    "    overlap = 0\n",
    "    for i in range(X_high.shape[0]):\n",
    "        overlap += len(np.intersect1d(high_neighbors[i], low_neighbors[i]))\n",
    "\n",
    "    continuity = overlap / (X_high.shape[0] * n_neighbors)\n",
    "    return continuity\n",
    "\n",
    "# Example usage:\n",
    "continuity_score = calculate_continuity(x_train, x_new_train)\n",
    "print(f\"Continuity: {continuity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress: 0.8691021885828404\n"
     ]
    }
   ],
   "source": [
    "def calculate_stress(X_high, X_low):\n",
    "    D_high = pairwise_distances(X_high)\n",
    "    D_low = pairwise_distances(X_low)\n",
    "\n",
    "    # Compute the stress as sum of squared differences of distances\n",
    "    stress = np.sum((D_high - D_low)**2) / np.sum(D_high**2)\n",
    "    return stress\n",
    "\n",
    "# Example usage:\n",
    "stress_score = calculate_stress(x_train, x_new_train)\n",
    "print(f\"Stress: {stress_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
